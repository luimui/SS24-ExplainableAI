{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Responsible Machine Learning Exercise 07 \n",
    "\n",
    "22.05.2023\n",
    "\n",
    "\n",
    "## 1. Feature Permutation Importance\n",
    "\n",
    "### Pen and Paper\n",
    "Calculate the feature permutation importance of the number of years of education given the below data and the linear model for income:\n",
    "$$\\hat{y} = 2 \\cdot Education + 1 \\cdot Height.$$\n",
    "\n",
    "|Height|Years of Education|Income (k)\n",
    "|-|-|-|\n",
    "|1.8 | 16 | 40\n",
    "|1.6 | 20 | 50\n",
    "|1.75 | 12 | 100\n",
    "|1.8 | 8 | 25\n",
    "\n",
    "Use the permutation $(1 \\rightarrow 3, 2\\rightarrow 4, 3 \\rightarrow 2, 4 \\rightarrow 1)$ and mean squared error.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculator cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding\n",
    "\n",
    "In this task, you will implement the feature permutation importance algorithm by filling in the parts in the following cells.\n",
    "Places where you have to write code are marked with `TODO`.\n",
    "\n",
    "To check whether your implementation is correct, you can use the [eli5 library](https://eli5.readthedocs.io/en/latest/index.html).\n",
    "To install it, open a shell/Anaconda Prompt and activate your Python environment.\n",
    "If you followed the instructions from the first exercise, you can use the following commands:\n",
    "\n",
    "```shell\n",
    "conda activate xai\n",
    "conda install -c conda-forge eli5\n",
    "```\n",
    "\n",
    "Afterwards, you should be able to import from eli5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, List, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "dataset = fetch_california_housing()\n",
    "print(\n",
    "    f\"We predict the median house value using a linear regression model based on the following features:\"\n",
    ")\n",
    "print(dataset.feature_names)\n",
    "\n",
    "# Train a model and reserve testing data\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    dataset.data, dataset.target, random_state=0\n",
    ")\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "print(\"\\nThe model performs as follows:\")\n",
    "print(\n",
    "    f\"R2={model.score(x_test, y_test):.4f}, \"\n",
    "    f\"MSE={mean_squared_error(y_test, model.predict(x_test)):.4f}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_permutation_importance(\n",
    "    model: BaseEstimator,\n",
    "    x: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    feat_idx: int,\n",
    "    loss: Callable = mean_squared_error,\n",
    "    random_state: int = 100,\n",
    ") -> float:\n",
    "    \"\"\"Calculates the decrease in loss when using the original features and a permutated \n",
    "    feature column.\n",
    "\n",
    "    Args:\n",
    "        model (BaseEstimator): trained model\n",
    "        x (np.ndarray): original input features\n",
    "        y (np.ndarray): targets\n",
    "        feat_idx (int): the index of the column to permute\n",
    "        loss (Callable, optional): a loss function. Defaults to mean_squared_error.\n",
    "        random_state (int, optional): seeds the permutation. Defaults to 100.\n",
    "\n",
    "    Returns:\n",
    "        float: difference of original loss and loss on permuted data\n",
    "    \"\"\"\n",
    "    # TODO: Fill in the function.\n",
    "    pass\n",
    "\n",
    "\n",
    "# TODO: Write a comment to explain the purpose of the rest of this cell.\n",
    "results = []\n",
    "for i, feature_name in enumerate(dataset.feature_names):\n",
    "    imps = np.array(\n",
    "        [\n",
    "            feature_permutation_importance(model, x_test, y_test, i, random_state=j)\n",
    "            for j in range(5)\n",
    "        ]\n",
    "    )\n",
    "    results.append((i, imps.mean(), imps.std()))\n",
    "\n",
    "for i, mean, std in sorted(results, key=lambda x: x[1], reverse=True):\n",
    "    print(f\"x{i}:{dataset.feature_names[i]}, importance={mean:.4f} Â± {std:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare your result to the implementation in eli5.\n",
    "\n",
    "* If the weight/importance score of a feature is close to zero, what does it mean w.r.t. to usage of the feature by the model for its predictions?\n",
    "\n",
    "* What is the most important feature?\n",
    "\n",
    "* What effect does removing the most important feature have on the model performance? Does it have an intuitive explanation? Describe why or why not the score makes sense.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "importance = PermutationImportance(\n",
    "    estimator=model,\n",
    "    scoring=make_scorer(mean_squared_error, greater_is_better=False),\n",
    "    random_state=0,\n",
    ").fit(x_test, y_test)\n",
    "eli5.show_weights(importance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PDP Plot\n",
    "\n",
    "### Coding\n",
    "\n",
    "Create the PDP plot of the model $\\hat{f}(x) = 100 \\cdot Height - 1 \\cdot Weight$ for the weight feature:\n",
    "Use the following values for weight: [50, 60, 70, 80, 90].\n",
    "\n",
    "You can do it manually or by implementing the method (recommended).\n",
    "\n",
    "|Height|Weight|Walking speed\n",
    "|-|-|-|\n",
    "|1.8 | 80 | 110\n",
    "|1.6 | 80 | 90\n",
    "|1.75 | 65 | 100\n",
    "|1.8 | 70 | 120\n",
    "|1.6 | 50 | 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(height: float, weight: float) -> float:\n",
    "    \"\"\"Return $\\hat{f}(x) = 100 \\cdot Height - 1 \\cdot Weight$\"\"\"\n",
    "    # TODO\n",
    "    pass\n",
    "\n",
    "\n",
    "def pdp_plot(\n",
    "    weight_grid: List[Union[float, int]], heights: List[Union[float, int]]\n",
    ") -> None:\n",
    "    \"\"\"Create a PDP plot for the specific problem in this task.\n",
    "\n",
    "    Args:\n",
    "        weight_grid (List[Union[float, int]]): The values of weight for which the predictions are calculated\n",
    "        heights (List[Union[float, int]]): The height values of the dataset\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    # You can use matplotlib.pyplot for plotting\n",
    "    pass\n",
    "\n",
    "\n",
    "weights = [80, 80, 65, 70, 50]\n",
    "heights = [1.8, 1.6, 1.75, 1.8, 1.6]\n",
    "\n",
    "pdp_plot(weight_grid=[50, 60, 70, 80, 90], heights=heights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting PDP Plots\n",
    "\n",
    "We now train a more complex model (a random forest) on the housing dataset used earlier.\n",
    "Using the inspection capabilities of sklearn, we can easily plot PDP plots (https://scikit-learn.org/stable/modules/partial_dependence.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(random_state=0)\n",
    "rf.fit(x_train, y_train)\n",
    "\n",
    "print(\"The model performs as follows:\")\n",
    "print(\n",
    "    f\"R2={rf.score(x_test, y_test):.4f}, \"\n",
    "    f\"MSE={mean_squared_error(y_test, rf.predict(x_test)):.4f}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 4, figsize=(8, 4))\n",
    "PartialDependenceDisplay.from_estimator(\n",
    "    rf,\n",
    "    x_test,\n",
    "    [i for i in range(len(dataset.feature_names))],\n",
    "    feature_names=dataset.feature_names,\n",
    "    ax=axes\n",
    ")\n",
    "for i, ax in enumerate(axes[0]):\n",
    "    ax.set_title(dataset.feature_names[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the features, describe their effect on the prediction and argue whether they are important for the prediction.\n",
    "\n",
    "What is a problem here w.r.t. to the data points we use to create these plots?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "67a89408c1fac786cceffe2773e5e0f65955f412963f9e9a2922d236112dd527"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
