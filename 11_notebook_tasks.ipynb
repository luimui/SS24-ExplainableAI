{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "We train a logistic regression classifier on the handwritten digits datasets.\n",
    "We modify the dataset slightly by changing it to a binary classification problem: all digits below 5 belong to one class, the rest to another.\n",
    "The code is based on https://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html.\n",
    "\n",
    "If you run the cell, you should see that the model performs quite well with 86% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()\n",
    "digits.target = (digits.target >= 5).astype(\n",
    "    int\n",
    ")  # modification into binary classification\n",
    "_, axes = plt.subplots(nrows=1, ncols=8, figsize=(10, 3))\n",
    "for ax, image, label in zip(axes, digits.images, digits.target):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "    ax.set_title(\"Training: %i\" % label)\n",
    "\n",
    "# flatten the images\n",
    "n_samples = len(digits.images)\n",
    "data = digits.images.reshape((n_samples, -1))\n",
    "\n",
    "# Create a classifier\n",
    "clf = LogisticRegression(random_state=0, max_iter=2000, penalty=\"none\")\n",
    "\n",
    "# Split data into 50% train and 50% test subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, digits.target, test_size=0.5, shuffle=False\n",
    ")\n",
    "\n",
    "# Learn the digits on the train subset\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the value of the digit on the test subset\n",
    "predicted = clf.predict(X_test)\n",
    "\n",
    "_, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 3))\n",
    "for ax, image, prediction in zip(axes, X_test, predicted):\n",
    "    ax.set_axis_off()\n",
    "    image = image.reshape(8, 8)\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "    ax.set_title(f\"Prediction: {prediction}\")\n",
    "\n",
    "print(\n",
    "    f\"Classification report for classifier {clf}:\\n\"\n",
    "    f\"{metrics.classification_report(y_test, predicted)}\\n\"\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast Gradient Sign Method\n",
    "\n",
    "Given the gradient that you calculated (Task 1.1 on the pdf), implement the FSGM method in the following function.\n",
    "Use the fact that the gradient of the loss function (cross entropy) with respect to the input is:\n",
    "\\begin{equation}\n",
    "\\nabla_x \\mathcal{L}(x,y,w) = \\left(\\frac{1 - y}{1 - \\hat{y}} - \\frac{y}{\\hat{y}}\\right) \\nabla_x \\hat{y},\n",
    "\\end{equation}\n",
    "where $\\hat{y} = \\sigma(x,w)$ is short for the logistic sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_x_sigmoid(x: np.ndarray, w: np.ndarray, b: float) -> np.ndarray:\n",
    "    # TODO: implement the gradient of the sigmoid function wrt input x\n",
    "\n",
    "\n",
    "def sigmoid(x: np.ndarray, w: np.ndarray, b: float) -> np.ndarray:\n",
    "    \"\"\"Logistic sigmoid function. Can be used as a helper function\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): input features\n",
    "        w (np.ndarray): weights\n",
    "        b (float): bias\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: logistic sigmoid(w.T @ x)\n",
    "    \"\"\"\n",
    "    # clip inputs to exp to avoid numerical problems\n",
    "    return 1 / (1 + np.exp(-np.clip(w.T @ x - b, -50, 50))) \n",
    "\n",
    "\n",
    "def grad_x_CEloss(x: np.ndarray, y: int, w: np.ndarray, b: float) -> np.ndarray:\n",
    "    eps = 1e-5  # add a small constant in denominator to avoid division by zero\n",
    "    # TODO: implement the gradient of the cross entropy loss with respect to x\n",
    "\n",
    "\n",
    "def fgsm_example(x: np.ndarray, y: int, clf, eps: float = 0.1) -> np.ndarray:\n",
    "    \"\"\"Generate an adversarial example with FGSM\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): input data point\n",
    "        y (int): binary class label\n",
    "        clf: sklearn logistic regression model\n",
    "        eps (float, optional): Allowed perturbation. Defaults to 0.1.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: perturbed image, same shape as x\n",
    "    \"\"\"\n",
    "    # TODO (use the clf.coef_ and clf.intercept_ to get the weights)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your implementation in the following cell with different levels of perturbation `eps`.\n",
    "It gives you a performance summary of the model on the original data and the perturbed data as well as visualizations of the original and perturbed images.\n",
    "* Can you make out the difference between original and perturbed image?\n",
    "* At what level of perturbation does the performance really start to decrease?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(clf, X_test_orig, X_test_perturbed):\n",
    "    predicted = clf.predict(X_test_orig)\n",
    "    predicted_perturbed = clf.predict(X_test_perturbed)\n",
    "\n",
    "    print(\n",
    "        f\"Classification report for classifier {clf} with original input:\\n\"\n",
    "        f\"{metrics.classification_report(y_test, predicted)}\\n\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Classification report for classifier {clf} with perturbed inputs:\\n\"\n",
    "        f\"{metrics.classification_report(y_test, predicted_perturbed)}\\n\"\n",
    "    )\n",
    "\n",
    "    diffs = predicted != predicted_perturbed\n",
    "    print(f\"{diffs.sum()} out of {len(diffs)} differently predicted data points\")\n",
    "\n",
    "    n_cols = 5  # the number of predictions shown\n",
    "    _, axes = plt.subplots(nrows=2, ncols=n_cols, figsize=(n_cols * 2.2, 3))\n",
    "    for j, image_orig, image_pert, pred_orig, pred_perturbed in zip(\n",
    "        range(n_cols),\n",
    "        X_test_orig[diffs],\n",
    "        X_test_perturbed[diffs],\n",
    "        predicted[diffs],\n",
    "        predicted_perturbed[diffs],\n",
    "    ):\n",
    "        axes[0, j].set_axis_off()\n",
    "        axes[1, j].set_axis_off()\n",
    "        image_orig = image_orig.reshape(8, 8)\n",
    "        image_pert = image_pert.reshape(8, 8)\n",
    "        axes[0, j].imshow(image_orig, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "        axes[1, j].imshow(image_pert, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "        axes[0, j].set_title(f\"Pred (original): {pred_orig}\")\n",
    "        axes[1, j].set_title(f\"Pred (perturbed): {pred_perturbed}\")\n",
    "\n",
    "\n",
    "# TODO\n",
    "eps = 0.2  # allowed change per pixel\n",
    "X_test_perturbed = np.array([fgsm_example(x, y, clf, eps=eps) for x, y in zip(X_test, y_test)])\n",
    "report(clf, X_test, X_test_perturbed)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adverserial Training\n",
    "\n",
    "Install `tqdm`, a package for progress bars, and `ipywidgets` in your environment: \n",
    "```shell\n",
    "conda activate xai\n",
    "conda install tqdm ipywidgets\n",
    "```\n",
    "\n",
    "Afterwards, restart the notebook kernel and execute all cells again.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell implements and trains a logistic regression model.\n",
    "For a robust model that is not easily fooled by perturbed images, we want to solve the following problem:\n",
    "\\begin{equation}\n",
    "\\min_w \\sum_{x,y} \\max_{\\delta \\in N} f_w(x + \\delta, y),\n",
    "\\end{equation}\n",
    "where $N$ are the allowed perturbations.\n",
    "\n",
    "**Your task is to implement adversarial training to create a model robust to FGSM.**\n",
    "To do this, add code in the following cell marked with TODO. In the class, you only have to modify `fit_fgsm_robust`.\n",
    "\n",
    "Train a standard model and a robust model and compare their performance on FGSM perturbed data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticClassifier:\n",
    "    def __init__(self) -> None:\n",
    "        self.w = None\n",
    "\n",
    "    @property\n",
    "    def coef_(self):\n",
    "        return self.w[:-1]\n",
    "\n",
    "    @property\n",
    "    def intercept_(self):\n",
    "        # print(self.w[-1], np.asarray(self.w[-1]).reshape(-1,))\n",
    "        return np.asarray(self.w[-1]).reshape(-1,)\n",
    "\n",
    "    def _add_constant(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Add a constant column to a matrix.\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): Original data matrix\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Original data matrix with concatenated column of all ones.\n",
    "        \"\"\"\n",
    "        return np.hstack((X, np.ones((len(X), 1))))\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        learning_rate: float = 1e-4,\n",
    "        n_epochs: int = 500,\n",
    "        random_state: int = 42,\n",
    "    ) -> None:\n",
    "        \"\"\"Fit the parameters of the model to the data with gradient descent.\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): features\n",
    "            y (np.ndarray): targets\n",
    "            learning_rate (float): step size of gradient descent\n",
    "            n_epochs (int): number of parameter updates\n",
    "            random_state (int): seed for reproducibility\n",
    "        \"\"\"\n",
    "        # initialize randomly\n",
    "        rng = np.random.default_rng(random_state)\n",
    "        self.w = rng.standard_normal(size=(X.shape[1] + 1,))  # +1 for bias\n",
    "        # print(self.w.shape)\n",
    "\n",
    "        # gradient descent\n",
    "        for _ in tqdm(range(n_epochs)):\n",
    "            self.w = self.w - learning_rate * self._gradient(X, y)\n",
    "            if np.isnan(self.w).sum() > 0:\n",
    "                raise ValueError(\"Weights have diverged\", self.w)\n",
    "\n",
    "    def fit_fgsm_robust(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        learning_rate: float = 1e-4,\n",
    "        n_epochs: int = 500,\n",
    "        random_state: int = 42,\n",
    "        perturbation_eps: float = 0.5,\n",
    "    ) -> None:\n",
    "        \"\"\"Fit the parameters of the model to FGSM-perturbed data with gradient descent.\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): features\n",
    "            y (np.ndarray): targets\n",
    "            learning_rate (float): step size of gradient descent\n",
    "            n_epochs (int): number of parameter updates\n",
    "            random_state (int): seed for reproducibility\n",
    "            perturbation_eps (float): level of allowed perturbation\n",
    "        \"\"\"\n",
    "        # initialize randomly\n",
    "        rng = np.random.default_rng(random_state)\n",
    "        self.w = rng.standard_normal(size=(X.shape[1] + 1,))  # +1 for bias\n",
    "        # print(self.w.shape)\n",
    "\n",
    "        # gradient descent\n",
    "        for _ in tqdm(range(n_epochs)):\n",
    "            # TODO: implement the maximization step of the formula in the cell above\n",
    "            \n",
    "            if np.isnan(self.w).sum() > 0:\n",
    "                raise ValueError(\"Weights have diverged\", self.w)\n",
    "\n",
    "    def _gradient(self, X: np.ndarray, y: np.ndarray):\n",
    "        grad = sum(\n",
    "            [\n",
    "                (yhat - y) * x\n",
    "                for x, y, yhat in zip(self._add_constant(X), y, self.predict_proba(X))\n",
    "            ]\n",
    "        )\n",
    "        return grad\n",
    "\n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Use parameters to predict values\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): features\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: predicted probablities\n",
    "        \"\"\"\n",
    "        X = self._add_constant(X)\n",
    "        probas = []\n",
    "        for x in X:\n",
    "            probas.append(1 / (1 + np.exp(-self.w.T @ x)))\n",
    "        probas = np.array(probas)\n",
    "        return probas\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Use parameters to predict values (hard predictions)\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): features\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: predicted class\n",
    "        \"\"\"\n",
    "        threshold = 0.5\n",
    "        X = self._add_constant(X)\n",
    "        probas = []\n",
    "        for x in X:\n",
    "            probas.append(1 / (1 + np.exp(-self.w.T @ x)))\n",
    "        probas = np.array(probas)\n",
    "        return (probas > threshold).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add training of a standard and a robust model\n",
    "clf = ...\n",
    "clf_robust = ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output for standard classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.6  # allowed change per pixel\n",
    "X_test_perturbed = np.array([fgsm_example(x, y, clf, eps=eps) for x, y in zip(X_test, y_test)])\n",
    "predicted_perturbed = clf.predict(X_test_perturbed)\n",
    "report(clf, X_test, X_test_perturbed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output for robust classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.6  # allowed change per pixel\n",
    "X_test_perturbed = np.array([fgsm_example(x, y, clf_robust, eps=eps) for (x, y) in zip(X_test, y_test)])\n",
    "predicted_perturbed = clf_robust.predict(X_test_perturbed)\n",
    "report(clf_robust, X_test, X_test_perturbed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projection based gradient descent\n",
    "\n",
    "Now, we implement a different adversarial attack, projection based gradient descent (PGD).\n",
    "While FGSM only takes a single step, PGD takes many smaller steps and always projects updates back into the $l_\\infty$ $\\varepsilon$-neighborhood of x by clipping the updates.\n",
    "\n",
    "* Add the code at the TODO. (Hint: You can reuse much of the code from before.)\n",
    "* How does the adversarially robust classifier perform on the PGD data?\n",
    "\n",
    "Solution: When correctly specifying the PGD attack, the robust model is barely better than the standard model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pgd_example(\n",
    "    x: np.ndarray,\n",
    "    y: int,\n",
    "    clf,\n",
    "    eps: float = 0.4,\n",
    "    n_steps: int = 10,\n",
    "    step_size: float = 0.1,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Generate an adversarial example with projected gradient descent (PGD)\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): input data point\n",
    "        y (int): binary target label\n",
    "        clf: sklearn logistic regression model\n",
    "        eps (float, optional): Allowed perturbation. Defaults to 0.4.\n",
    "        n_steps (int, optional): Number of gradient descent steps. Defaults to 10.\n",
    "        step_size(float, optional): Step size. Defaults to 0.1\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: perturbed image, same shape as x\n",
    "    \"\"\"\n",
    "    # TODO (use the clf.coef_ to get the weights)\n",
    "    \n",
    "\n",
    "\n",
    "eps = 0.4  # allowed change per pixel\n",
    "X_test_perturbed = np.array(\n",
    "    [\n",
    "        pgd_example(x, y, clf_robust, eps=eps, n_steps=10, step_size=0.05)\n",
    "        for x, y in zip(X_test, y_test)\n",
    "    ]\n",
    ")\n",
    "report(clf_robust, X_test, X_test_perturbed)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze how the robust and standard classifier perform for PGD and FGSM attacks on various levels of epsilon by creating a plot that shows values of epsilon on the x-axis and accuracy of the classifier on the y-axis.\n",
    "\n",
    "What can you observe for the robust classifier and FGSM attacks?\n",
    "\n",
    "Solution: As noted before the robust classifier performs barely better than the standard model for PGD. For FGSM, the robust model performs worse than the standard model on clean data. It is overfit on FGSM data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: PGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: FGSM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "67a89408c1fac786cceffe2773e5e0f65955f412963f9e9a2922d236112dd527"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
