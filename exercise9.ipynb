{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N3TX2dVWTpr"
      },
      "source": [
        "# Exercise 9\n",
        "In this exercise, you will learn about Grad-CAM, guided gradients, Grad-CAM++, and integrated gradients. You will also learn how to use these methods for trained PyTorch models.<br>\n",
        "References:<br>\n",
        "[Grad-Cam](https://towardsdatascience.com/grad-cam-in-pytorch-use-of-forward-and-backward-hooks-7eba5e38d569)<br>\n",
        "[pytorch-grad-cam](https://github.com/vickyliin/gradcam_plus_plus-pytorch)<br>\n",
        "[Captum](https://captum.ai/)<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZF2wb9VWTps"
      },
      "source": [
        "#### Setting up deep neural network:\n",
        "In this exercise, we use a pretrained deep neural network, ResNet50, which is trained for image classification. This model is trained on the ImageNet dataset and has 1000 different classes for prediction. For more information about this model, you can refer to this link:\n",
        "[Resnet50](https://huggingface.co/microsoft/resnet-50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQOYEqpdWTps"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leYYPBgAWTpt"
      },
      "source": [
        "#### Import libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtWU7x6aWTpt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchsummary import summary\n",
        "from torchvision import models, transforms\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "from skimage.io import imread\n",
        "from skimage.transform import resize\n",
        "import torch.nn.functional as F\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "import PIL\n",
        "from matplotlib import colormaps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEEaQQ5RWTpt",
        "outputId": "a252fa89-2437-4891-f1ce-ad16fa359652"
      },
      "outputs": [],
      "source": [
        "model = models.resnet50(weights=\"ResNet50_Weights.IMAGENET1K_V1\")\n",
        "model.eval();\n",
        "print(summary(model, (3, 224, 224)))\n",
        "\n",
        "# This function is defined for reconstructing the real image from a normalized image\n",
        "inv_normalize = transforms.Normalize(\n",
        "    mean=[-0.485/0.229, -0.456/0.224, -0.406/0.255],\n",
        "    std=[1/0.229, 1/0.224, 1/0.255]\n",
        ")\n",
        "\n",
        "# function for normalizing the image:\n",
        "def preprocess_image(image_path):\n",
        "    img = Image.open(image_path)\n",
        "    preprocess = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    img_tensor = preprocess(img).unsqueeze(0)\n",
        "    return img_tensor\n",
        "\n",
        "#This function will plot the real image alongside the attribution image:\n",
        "def plot(input_img, attribution):\n",
        "    #plot image and its saleincy map\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(np.transpose(input_img, (1, 2, 0)))\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(attribution, cmap=plt.cm.hot)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dfeOrfpWTpt"
      },
      "source": [
        "## Grad CAM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibpZLG84WTpt"
      },
      "source": [
        "### What is grad-cam:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfrFIk5VWTpt"
      },
      "source": [
        "![Google Drive Image](https://drive.google.com/uc?export=view&id=1lVKRdjyjioy2TynKWbZ8LPp4-nc4o8iR)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Zd9AbWvWTpt"
      },
      "source": [
        "In the Grad-CAM approach, the objective is to determine which aspects of the input image are encoded by a middle layer of a convolutional neural network (CNN), and how it represents these features for a given image. <br><br>\n",
        "To achieve this, the method initially forwards the test image through the deep neural network to find the value that maximizes the output classification vector for that image, and also retrieves the values of the feature maps $(A_k)$. <br>\n",
        "Next, it calculates the gradient of that score with respect to the target activation feature map $( A_k )$ and computes the mean of gradients in each channel of the feature map. These averaged gradients are denoted by $( \\delta_i^c )$. Subsequently, these $( \\delta_i^c )$ values serve as weights for the activation function, and a weighted sum/mean of $( A_k )$ is computed. Applying the ReLU function to the resultant image yields the Grad-CAM image.<br><br> Afterwards, the Grad-CAM image can be resized using interpolation and overlaid onto the original image for visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAOfWJbCWTpt",
        "outputId": "334927c6-0bf8-40d2-87f6-7c3908cc7e78"
      },
      "outputs": [],
      "source": [
        "# download test image:\n",
        "!wget \"https://wallpapercave.com/wp/7GnAy3T.jpg\" -O \"car1.jpg\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUTX0miAWTpt"
      },
      "source": [
        "### Implementing grad-cam from scratch:\n",
        "To calculate gradients and feature maps, you can use the register_full_backward_hook and register_forward_hook functions in PyTorch:\n",
        "#### register_full_backward_hook:\n",
        " This method registers a backward hook on the module, which means that the hook function will run when the backward() method is called.\n",
        "The backward hook function receives as inputs the module itself, the gradients with respect to the layer’s input, and the gradients with respect to the layer’s output.\n",
        "\n",
        "#### register_forward_hook:\n",
        "This is quite similar to the previous one, except that the hook function runs in the forward pass, i.e., when the layer of interest processes its input and returns its outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "B9bgGnn4WTpt",
        "outputId": "8d46c838-0b04-41e7-a69e-e49f7ca6e4dc"
      },
      "outputs": [],
      "source": [
        "gradients = None\n",
        "activations = None\n",
        "model = models.resnet50(weights=\"ResNet50_Weights.IMAGENET1K_V1\")\n",
        "model.eval();\n",
        "\n",
        "def backward_hook(module, grad_input, grad_output):\n",
        "  global gradients\n",
        "  gradients = grad_output\n",
        "\n",
        "\n",
        "def forward_hook(module, args, output):\n",
        "  global activations\n",
        "  activations = output\n",
        "\n",
        "\n",
        "test_image = preprocess_image(\"car1.jpg\")\n",
        "\n",
        "#########################\n",
        "# TODO bind your backward_hook and forward_hook to the last element of layer4 of your model\n",
        "# for_hook = \n",
        "# back_hook = \n",
        "########################\n",
        "\n",
        "\n",
        "#########################\n",
        "#TODO calculate the prediction of resnet model for test_image and find max value of score\n",
        "# after that call backward() for max value\n",
        "# find mean of gradient for each chanels of feature map (the gradients are in gradients)\n",
        "# multiply each channel of activation to these mean values and summ all of activation maps\n",
        "# feed results into relu function and save result in heatmap\n",
        "\n",
        "\n",
        "# heatmap = \n",
        "##########################\n",
        "\n",
        "\n",
        "\n",
        "heatmap /= torch.max(heatmap)\n",
        "plt.imshow(heatmap.detach().numpy())\n",
        "for_hook.remove()\n",
        "back_hook.remove()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "THGaYvW1WTpu",
        "outputId": "a55a01c1-b8c8-47b6-f1bb-e508a9b1edc8"
      },
      "outputs": [],
      "source": [
        "real_image = inv_normalize(preprocess_image(\"car1.jpg\")[0])\n",
        "overlay = to_pil_image(heatmap.detach(), mode='F').resize((224,224), resample=PIL.Image.BICUBIC)\n",
        "cmap = colormaps['jet']\n",
        "overlay = (255 * cmap(np.asarray(overlay))[:, :, :3]).astype(np.uint8)\n",
        "plot(real_image,overlay)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CI6e-0u3WTpu"
      },
      "source": [
        "### Question:\n",
        "1. What is the difference between the vanilla saliency map and the Grad-CAM method?\n",
        "2. Change the target CNN layer from 4 to 1 and run the code again. Explain why the gradient changes in this manner."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtLGD7VpWTpu"
      },
      "source": [
        "### Using pytorch grad-cam lib:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJ0WY21FWTpu",
        "outputId": "87847593-163d-484f-a42c-2c748685c88d"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch-gradcam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "id": "UPqzaXBaWTpu",
        "outputId": "2f004399-26d4-4c15-8eea-6c19bd14d15f"
      },
      "outputs": [],
      "source": [
        "from gradcam import GradCAM\n",
        "from gradcam.utils import visualize_cam\n",
        "\n",
        "model = models.resnet50(weights=\"ResNet50_Weights.IMAGENET1K_V1\")\n",
        "model.eval();\n",
        "\n",
        "gradcam = GradCAM(model,model.layer4[-1])\n",
        "torch_img = preprocess_image(\"car1.jpg\")\n",
        "real_image = inv_normalize(preprocess_image(\"car1.jpg\")[0])\n",
        "\n",
        "mask, _ = gradcam(torch_img)\n",
        "heatmap, result = visualize_cam(mask, torch_img)\n",
        "plot(real_image,np.transpose(heatmap, (1, 2, 0)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tp5CBFSxWTpu"
      },
      "source": [
        "### Guided gradients:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3k8TZAkYdpI",
        "outputId": "c78e9650-6612-4a11-c8fd-3fda0684ad9d"
      },
      "outputs": [],
      "source": [
        "!pip install captum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 920
        },
        "id": "utm4Ta8XWTpu",
        "outputId": "c3493733-e3bd-4772-a971-54749ac9c832"
      },
      "outputs": [],
      "source": [
        "from captum.attr import InputXGradient,GuidedBackprop\n",
        "\n",
        "\n",
        "\n",
        "#TODO###############\n",
        "# use captum InputXGradient and plot the input gradient with respect to indices[0]\n",
        "\n",
        "# attribution = \n",
        "####################\n",
        "\n",
        "\n",
        "#TODO###############\n",
        "# use captum GuidedBackprop and plot the guided gradients with respect to indices[0]\n",
        "# attribution_guided = \n",
        "####################\n",
        "\n",
        "\n",
        "plot(real_image,np.transpose(attribution[0].detach().numpy()*10, (1,2,0)))\n",
        "plot(real_image,np.transpose(attribution_guided[0].detach().numpy()*10, (1,2,0)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xXdLOxVWTpu"
      },
      "source": [
        "### Question:\n",
        "1. Explain the difference between input gradients and guided gradients?\n",
        "look at this [Link](https://medium.com/@chinesh4/generalized-way-of-interpreting-cnns-a7d1b0178709)\n",
        "2. How does guided gradient help to provide better attribution?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFDSr8XAWTpu"
      },
      "source": [
        "## Grad-Cam ++"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8h1a6PnWTpu",
        "outputId": "a22b7f6b-d715-4e34-d7fa-936374227541"
      },
      "outputs": [],
      "source": [
        "from captum.attr import GuidedGradCam\n",
        "\n",
        "guided_gc = GuidedGradCam(model, model.layer4[-1])\n",
        "attribution_gradcam_guided = guided_gc.attribute(torch_img, indices[0])\n",
        "\n",
        "plot(real_image,np.transpose(attribution_gradcam_guided[0].detach().numpy()*5, (1,2,0)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGiEybHLWTpu"
      },
      "source": [
        "## Question:\n",
        "1. Can you explain the difference between GradCAM and GradCAM++?\n",
        "2. How does GradCAM++ help to provide better attribution?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEr6CHFDWTpu"
      },
      "source": [
        "## Integrated Gradients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpzQR1_jWTpu"
      },
      "source": [
        "### Question:\n",
        "please read this [Link](https://arxiv.org/abs/1703.01365) and answer the following questions:\n",
        "1. What is Sensivity and Implementation Invariance in atributions methods?<br>\n",
        "2. Explain how IG method work?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0G59MsmWTpu"
      },
      "outputs": [],
      "source": [
        "!pip install captum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fToSb9ePWTpu"
      },
      "outputs": [],
      "source": [
        "from captum.attr import IntegratedGradients\n",
        "model = models.resnet50(weights=\"ResNet50_Weights.IMAGENET1K_V1\")\n",
        "\n",
        "torch_img = preprocess_image(\"car1.jpg\")\n",
        "real_image = inv_normalize(preprocess_image(\"car1.jpg\")[0])\n",
        "\n",
        "\n",
        "model.eval();\n",
        "preds = model(test_image)\n",
        "score, indices = torch.max(preds, 1)\n",
        "integrated_gradients = IntegratedGradients(model)\n",
        "attributions_ig = integrated_gradients.attribute(torch_img, target=indices[0], n_steps=200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWQCfBJ6WTpu",
        "outputId": "273ebd47-c368-45b6-d3e2-d56d2b363c81"
      },
      "outputs": [],
      "source": [
        "from captum.attr import visualization as viz\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "\n",
        "default_cmap = LinearSegmentedColormap.from_list('custom blue',\n",
        "                                                 [(0, '#ffffff'),\n",
        "                                                  (0.25, '#000000'),\n",
        "                                                  (1, '#000000')], N=256)\n",
        "_ = viz.visualize_image_attr_multiple(np.transpose(attributions_ig.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
        "                                      np.transpose(torch_img.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
        "                                      [\"original_image\", \"heat_map\"],\n",
        "                                      [\"all\", \"positive\"],\n",
        "                                      cmap=default_cmap,\n",
        "                                      show_colorbar=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWz_nRhRWTpu"
      },
      "outputs": [],
      "source": [
        "from captum.attr import NoiseTunnel\n",
        "\n",
        "noise_tunnel = NoiseTunnel(integrated_gradients)\n",
        "\n",
        "attributions_ig_nt = noise_tunnel.attribute(torch_img, nt_samples=10, nt_type='smoothgrad_sq', target=indices[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwZm5ytoWTpu",
        "outputId": "ff27e75e-5efe-4873-e86d-7e95c17e9514"
      },
      "outputs": [],
      "source": [
        "default_cmap = LinearSegmentedColormap.from_list('custom blue',\n",
        "                                                 [(0, '#ffffff'),\n",
        "                                                  (0.25, '#000000'),\n",
        "                                                  (1, '#000000')], N=256)\n",
        "_ = viz.visualize_image_attr_multiple(np.transpose(attributions_ig_nt.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
        "                                      np.transpose(torch_img.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
        "                                      [\"original_image\", \"heat_map\"],\n",
        "                                      [\"all\", \"positive\"],\n",
        "                                      cmap=default_cmap,\n",
        "                                      show_colorbar=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "XAI",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
